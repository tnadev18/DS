# -*- coding: utf-8 -*-
"""ds_practical_07.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s6v0KTgurRlZPUSLGiKwXXS3DeF7g0Mj
"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag
from sklearn.feature_extraction.text import TfidfVectorizer

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Load the text from the document
with open('datasets/nlp_text.txt', 'r') as file:
    text = file.read()

# Tokenization
tokens = word_tokenize(text)

# Print results
print("Tokenization:")
print(tokens)

# POS Tagging
pos_tags = pos_tag(tokens)

print("\nPOS Tagging:")
print(pos_tags)

# Stop words removal
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

print("\nStop Words Removal:")
print(filtered_tokens)

# Stemming and Lemmatization
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]

print("\nStemming:")
print(stemmed_tokens)

lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]

print("\nLemmatization:")
print(lemmatized_tokens)

# Calculate Term Frequency-Inverse Document Frequency (TF-IDF)
tfidf_vectorizer = TfidfVectorizer()
tfidf_representation = tfidf_vectorizer.fit_transform([text])


print("\nTF-IDF Representation:")
print(tfidf_representation.toarray())